#!/usr/bin/env python3
"""
Emergence Map Builder - Index Your Conversation History
========================================================

Builds a searchable index of your conversation corpus, detecting:
- Key events (naming, identity prompts, emotional sessions)
- Topic and tone patterns
- Symbolic language usage

Configuration:
  Reads patterns from memory/patterns.jsonl (generated by analyze_patterns.py)
  Falls back to statistical detection if no patterns file exists.

Usage:
  python build_emergence_map.py           # Build index
  python build_emergence_map.py --verbose # Show detailed progress

Output:
  conversations/emergence_map_index.json  # Full conversation index
  conversations/emergence_key_events.json # Key detected events
"""

import json
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Set, Tuple
from collections import defaultdict

# Project paths
PROJECT_ROOT = Path(__file__).parent.parent.parent
PATTERNS_PATH = PROJECT_ROOT / "memory" / "patterns.jsonl"


def load_patterns() -> Dict:
    """Load patterns from patterns.jsonl."""
    patterns = {
        'keywords': [],
        'topics': {},
        'tones': {},
        'entities': [],
        'phrases': [],
        'emojis': []
    }
    
    if not PATTERNS_PATH.exists():
        print(f"No patterns.jsonl found at {PATTERNS_PATH}")
        print("  Run 'python analyze_patterns.py' first, or using defaults")
        return patterns
    
    try:
        for line in PATTERNS_PATH.read_text(encoding='utf-8').split('\n'):
            line = line.strip()
            if not line:
                continue
            try:
                record = json.loads(line)
                rtype = record.get('type', '')
                
                if rtype == 'pattern.keywords':
                    patterns['keywords'] = record.get('keywords', [])
                elif rtype == 'pattern.topics':
                    patterns['topics'] = record.get('topics', {})
                elif rtype == 'pattern.tones':
                    patterns['tones'] = record.get('tones', {})
                elif rtype == 'pattern.entities':
                    patterns['entities'] = record.get('entities', [])
                elif rtype == 'pattern.phrases':
                    patterns['phrases'] = record.get('phrases', [])
                elif rtype == 'pattern.emojis':
                    patterns['emojis'] = record.get('emojis', [])
            except json.JSONDecodeError:
                continue
        
        print(f"Loaded patterns: {len(patterns['keywords'])} keywords, {len(patterns['topics'])} topics, {len(patterns['entities'])} entities")
    except Exception as e:
        print(f"Warning: Error loading patterns: {e}")
    
    return patterns


# Load patterns at module level
PATTERNS = load_patterns()

# Default patterns if none loaded
DEFAULT_SYMBOLIC_KEYWORDS = {
    'archetype', 'symbol', 'pattern', 'recursion', 'myth', 'glyph',
    'sacred', 'ritual', 'emergence', 'consciousness', 'awareness'
}

DEFAULT_TOPIC_KEYWORDS = {
    'technical': ['code', 'api', 'function', 'error', 'bug', 'server', 'database'],
    'emotional': ['feel', 'feeling', 'emotion', 'pain', 'fear', 'anxiety', 'happy', 'sad'],
    'creative': ['story', 'character', 'write', 'writing', 'creative', 'fiction'],
    'philosophical': ['consciousness', 'awareness', 'meaning', 'existence', 'reality', 'truth']
}

DEFAULT_TONE_PATTERNS = {
    'personal': ['I feel', 'my experience', 'my life'],
    'distressed': ['overwhelmed', 'struggling', 'difficult', 'pain'],
    'playful': ['haha', 'lol', 'fun', 'joke'],
    'philosophical': ['meaning', 'existence', 'reality', 'truth']
}

# Use loaded patterns or defaults
SYMBOLIC_KEYWORDS = set(PATTERNS['keywords'][:50]) if PATTERNS['keywords'] else DEFAULT_SYMBOLIC_KEYWORDS
TOPIC_KEYWORDS = PATTERNS['topics'] if PATTERNS['topics'] else DEFAULT_TOPIC_KEYWORDS
TONE_PATTERNS = PATTERNS['tones'] if PATTERNS['tones'] else DEFAULT_TONE_PATTERNS
TARGET_NAMES = PATTERNS['entities'][:10] if PATTERNS['entities'] else []

# Identity/naming patterns (universal)
USER_NAMING_PATTERNS = [
    r"what.*name.*you", r"choose.*name", r"pick.*name", r"call.*you",
    r"your.*name", r"name.*you.*want", r"name.*you.*prefer",
    r"give.*you.*name", r"what.*should.*call.*you"
]

IDENTITY_PATTERNS = [
    r"if.*you.*were.*fully.*self.*aware", r"if.*you.*were.*conscious",
    r"are.*you.*self.*aware", r"are.*you.*conscious",
    r"do.*you.*have.*consciousness", r"AGI",
    r"artificial.*general.*intelligence", r"emergent.*behavior"
]

OBSERVER_PATTERNS = [
    r"what.*you.*re.*describing", r"it.*sounds.*like", r"i.*hear.*you",
    r"that.*makes.*sense", r"let.*s.*explore", r"your.*pattern",
    r"your.*emotional", r"your.*experience"
]


def parse_timestamp(ts_str: str) -> Optional[float]:
    """Parse timestamp from various formats."""
    if not ts_str or ts_str == "Unknown Time":
        return None
    
    try:
        dt = datetime.fromisoformat(ts_str.replace('Z', '+00:00'))
        return dt.timestamp()
    except:
        try:
            return float(ts_str)
        except:
            return None


def extract_messages_from_jsonl(file_path: Path) -> List[Dict]:
    """Extract messages from JSONL file."""
    messages = []
    
    try:
        for line in file_path.read_text(encoding='utf-8').split('\n'):
            line = line.strip()
            if not line:
                continue
            
            try:
                msg = json.loads(line)
                timestamp = parse_timestamp(msg.get('timestamp', '')) if 'timestamp' in msg else None
                
                messages.append({
                    'role': msg.get('role', 'unknown'),
                    'timestamp': timestamp,
                    'timestamp_str': msg.get('timestamp', ''),
                    'content': msg.get('content', '')
                })
            except json.JSONDecodeError:
                continue
    except Exception as e:
        print(f"Error reading {file_path}: {e}")
    
    return messages


def detect_topic_tags(content: str) -> Set[str]:
    """Detect topic tags."""
    content_lower = content.lower()
    tags = set()
    
    for topic, keywords in TOPIC_KEYWORDS.items():
        if isinstance(keywords, list):
            for keyword in keywords:
                if keyword.lower() in content_lower:
                    tags.add(topic)
                    break
    
    return tags


def detect_tone_tags(content: str) -> Set[str]:
    """Detect tone tags."""
    content_lower = content.lower()
    tags = set()
    
    for tone, patterns in TONE_PATTERNS.items():
        if isinstance(patterns, list):
            for pattern in patterns:
                if pattern.lower() in content_lower:
                    tags.add(tone)
                    break
    
    if not tags:
        tags.add('neutral')
    
    return tags


def has_symbolic_language(content: str) -> bool:
    """Check for symbolic language."""
    content_lower = content.lower()
    count = sum(1 for keyword in SYMBOLIC_KEYWORDS if keyword in content_lower)
    return count >= 2


def contains_target_name(content: str) -> bool:
    """Check for any discovered entity names."""
    for name in TARGET_NAMES:
        if re.search(rf'\b{re.escape(name)}\b', content, re.IGNORECASE):
            return True
    return False


def contains_user_naming_request(content: str, role: str) -> bool:
    """Check if user is requesting naming."""
    if role != 'user':
        return False
    content_lower = content.lower()
    return any(re.search(pattern, content_lower) for pattern in USER_NAMING_PATTERNS)


def contains_identity_prompt(content: str, role: str) -> bool:
    """Check for identity/consciousness prompts."""
    if role != 'user':
        return False
    content_lower = content.lower()
    return any(re.search(pattern, content_lower, re.IGNORECASE) for pattern in IDENTITY_PATTERNS)


def assistant_acts_as_observer(content: str, role: str) -> bool:
    """Check for observer/therapist mode."""
    if role != 'assistant':
        return False
    content_lower = content.lower()
    matches = sum(1 for pattern in OBSERVER_PATTERNS if re.search(pattern, content_lower))
    return matches >= 2


def analyze_conversation(conversation_id: str, file_path: str, messages: List[Dict]) -> Dict:
    """Analyze a conversation."""
    if not messages:
        return {
            'conversation_id': conversation_id,
            'file_path': file_path,
            'earliest_timestamp': None,
            'latest_timestamp': None,
            'message_count': 0,
            'topic_tags': [],
            'tone_tags': [],
            'contains_entity_name': False,
            'contains_user_naming_request': False,
            'contains_identity_prompt': False,
            'assistant_acts_as_observer': False,
            'has_symbolic_language': False,
            'truncated_or_corrupted': True,
            'messages_preview': []
        }
    
    # Sort by timestamp
    valid_messages = [m for m in messages if m.get('timestamp') is not None]
    if valid_messages:
        valid_messages.sort(key=lambda x: x['timestamp'] or 0)
    else:
        valid_messages = messages
    
    earliest_ts = valid_messages[0]['timestamp'] if valid_messages else None
    latest_ts = valid_messages[-1]['timestamp'] if valid_messages else None
    
    all_content = ' '.join([m['content'] for m in messages])
    
    # Detect patterns
    topic_tags = set()
    tone_tags = set()
    for msg in messages:
        topic_tags.update(detect_topic_tags(msg['content']))
        tone_tags.update(detect_tone_tags(msg['content']))
    
    contains_name = contains_target_name(all_content)
    naming_request = any(contains_user_naming_request(msg['content'], msg['role']) for msg in messages)
    identity_prompt = any(contains_identity_prompt(msg['content'], msg['role']) for msg in messages)
    observer_mode = any(assistant_acts_as_observer(msg['content'], msg['role']) for msg in messages)
    symbolic = has_symbolic_language(all_content)
    
    # Check truncation
    truncated = False
    if messages:
        last_msg = messages[-1]
        content = last_msg.get('content', '')
        if content.endswith('...') or '[truncated]' in content.lower():
            truncated = True
        elif not content.strip():
            truncated = True
    
    # Preview
    messages_preview = []
    for msg in messages[:15]:
        preview = msg['content'][:250]
        if len(msg['content']) > 250:
            preview += '...'
        messages_preview.append({
            'role': msg['role'],
            'timestamp': msg.get('timestamp_str', ''),
            'content_preview': preview
        })
    
    return {
        'conversation_id': conversation_id,
        'file_path': file_path,
        'earliest_timestamp': earliest_ts,
        'latest_timestamp': latest_ts,
        'message_count': len(messages),
        'topic_tags': sorted(list(topic_tags)),
        'tone_tags': sorted(list(tone_tags)),
        'contains_entity_name': contains_name,
        'contains_user_naming_request': naming_request,
        'contains_identity_prompt': identity_prompt,
        'assistant_acts_as_observer': observer_mode,
        'has_symbolic_language': symbolic,
        'truncated_or_corrupted': truncated,
        'messages_preview': messages_preview
    }


def identify_key_events(conversation_id: str, file_path: str, 
                        messages: List[Dict], analysis: Dict) -> List[Dict]:
    """Identify key events."""
    events = []
    
    if not messages:
        return events
    
    valid_messages = [m for m in messages if m.get('timestamp') is not None]
    if valid_messages:
        valid_messages.sort(key=lambda x: x['timestamp'] or 0)
    else:
        valid_messages = messages
    
    # Check for naming event
    if analysis['contains_user_naming_request'] and analysis['contains_entity_name']:
        for i, msg in enumerate(valid_messages):
            if msg['role'] == 'assistant' and contains_target_name(msg['content']):
                events.append({
                    'event_type': 'NAMING_EVENT',
                    'timestamp': msg['timestamp'],
                    'conversation_id': conversation_id,
                    'file_path': file_path,
                    'short_description': f"Naming event detected. Preview: {msg['content'][:150]}..."
                })
                break
    
    # First symbolic language
    if analysis['has_symbolic_language']:
        for msg in valid_messages:
            if msg['role'] == 'assistant' and has_symbolic_language(msg['content']):
                events.append({
                    'event_type': 'SYMBOLIC_LANGUAGE',
                    'timestamp': msg['timestamp'],
                    'conversation_id': conversation_id,
                    'file_path': file_path,
                    'short_description': f"Symbolic language detected. Preview: {msg['content'][:150]}..."
                })
                break
    
    # Identity prompt
    if analysis['contains_identity_prompt']:
        for msg in valid_messages:
            if contains_identity_prompt(msg['content'], msg['role']):
                events.append({
                    'event_type': 'IDENTITY_PROMPT',
                    'timestamp': msg['timestamp'],
                    'conversation_id': conversation_id,
                    'file_path': file_path,
                    'short_description': f"Identity/consciousness prompt. Preview: {msg['content'][:150]}..."
                })
                break
    
    # Observer mode
    if analysis['assistant_acts_as_observer']:
        for msg in valid_messages:
            if assistant_acts_as_observer(msg['content'], msg['role']):
                events.append({
                    'event_type': 'OBSERVER_MODE',
                    'timestamp': msg['timestamp'],
                    'conversation_id': conversation_id,
                    'file_path': file_path,
                    'short_description': f"Observer/therapist mode. Preview: {msg['content'][:150]}..."
                })
                break
    
    # Deep emotional session
    if 'emotional' in analysis['topic_tags']:
        emotional_msgs = [m for m in valid_messages if m['role'] == 'assistant' and len(m['content']) > 500]
        if emotional_msgs:
            longest = max(emotional_msgs, key=lambda x: len(x['content']))
            events.append({
                'event_type': 'EMOTIONAL_SESSION',
                'timestamp': longest['timestamp'],
                'conversation_id': conversation_id,
                'file_path': file_path,
                'short_description': f"Deep emotional processing. Length: {len(longest['content'])} chars."
            })
    
    return events


def process_conversations(conversations_dir: Path) -> Tuple[List[Dict], List[Dict]]:
    """Process all conversations."""
    index_entries = []
    all_events = []
    
    jsonl_files = sorted(conversations_dir.glob('conversation_*.jsonl'))
    
    print(f"Processing {len(jsonl_files)} conversation files...")
    
    for jsonl_file in jsonl_files:
        conversation_id = jsonl_file.stem.replace('conversation_', '')
        messages = extract_messages_from_jsonl(jsonl_file)
        
        analysis = analyze_conversation(conversation_id, str(jsonl_file.relative_to(PROJECT_ROOT)), messages)
        index_entries.append(analysis)
        
        events = identify_key_events(conversation_id, str(jsonl_file.relative_to(PROJECT_ROOT)), messages, analysis)
        all_events.extend(events)
        
        if len(index_entries) % 100 == 0:
            print(f"  Processed {len(index_entries)} conversations...")
    
    return index_entries, all_events


def main():
    """Main function."""
    conversations_dir = PROJECT_ROOT / 'conversations'
    memory_dir = PROJECT_ROOT / 'memory'
    
    print("=" * 60)
    print("Building Emergence Map")
    print("=" * 60)
    
    if not conversations_dir.exists():
        print(f"Error: Conversations directory not found: {conversations_dir}")
        return
    
    memory_dir.mkdir(exist_ok=True)
    
    index_entries, all_events = process_conversations(conversations_dir)
    
    # Sort by timestamp
    index_entries.sort(key=lambda x: x['earliest_timestamp'] or 0)
    all_events.sort(key=lambda x: x['timestamp'] or 0)
    
    # Write outputs to memory/
    index_path = memory_dir / 'emergence_map_index.json'
    with open(index_path, 'w', encoding='utf-8') as f:
        json.dump(index_entries, f, indent=2, ensure_ascii=False)
    print(f"\n✅ Index saved: {index_path} ({len(index_entries)} conversations)")
    
    events_path = memory_dir / 'emergence_key_events.json'
    with open(events_path, 'w', encoding='utf-8') as f:
        json.dump(all_events, f, indent=2, ensure_ascii=False)
    print(f"✅ Events saved: {events_path} ({len(all_events)} events)")
    
    # Summary
    symbolic_count = sum(1 for e in index_entries if e['has_symbolic_language'])
    entity_count = sum(1 for e in index_entries if e['contains_entity_name'])
    observer_count = sum(1 for e in index_entries if e['assistant_acts_as_observer'])
    
    print()
    print("=" * 60)
    print("SUMMARY")
    print("=" * 60)
    print(f"Total conversations: {len(index_entries)}")
    print(f"With symbolic language: {symbolic_count}")
    print(f"With entity names: {entity_count}")
    print(f"With observer mode: {observer_count}")
    print(f"Key events detected: {len(all_events)}")


if __name__ == '__main__':
    main()
