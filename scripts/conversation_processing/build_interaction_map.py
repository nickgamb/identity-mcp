#!/usr/bin/env python3
"""
Interaction Map Builder - Index Your Conversation History
==========================================================

Builds a searchable index of your conversation corpus, focusing on human
communication patterns: topics, tones, problem-solving moments, and
communication tempo/cadence changes.

Primary focus: Human identity fingerprinting - how you communicate, think,
and solve complex problems.

Configuration:
  Reads patterns from memory/patterns.jsonl (generated by analyze_patterns.py)
  Falls back to statistical detection if no patterns file exists.

Usage:
  python build_interaction_map.py           # Build index
  python build_interaction_map.py --verbose # Show detailed progress

Output:
  memory/interaction_map_index.json  # Full conversation index
  memory/interaction_key_events.json # Key human communication events
"""

import json
import os
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Set, Tuple
from collections import defaultdict

# Project paths
PROJECT_ROOT = Path(__file__).parent.parent.parent

# Support multi-user: read USER_ID from environment
USER_ID = os.environ.get("USER_ID")
def get_user_dir(base_dir: Path, user_id: Optional[str] = None) -> Path:
    """Get user-specific directory if user_id is provided, otherwise base directory."""
    if user_id:
        return base_dir / user_id
    return base_dir

MEMORY_DIR = get_user_dir(PROJECT_ROOT / "memory", USER_ID)
PATTERNS_PATH = MEMORY_DIR / "patterns.jsonl"


def load_patterns() -> Dict:
    """Load patterns from patterns.jsonl."""
    patterns = {
        'keywords': [],
        'topics': {},
        'tones': {},
        'entities': [],
        'phrases': [],
        'emojis': []
    }
    
    if not PATTERNS_PATH.exists():
        print(f"No patterns.jsonl found at {PATTERNS_PATH}")
        print("  Run 'python analyze_patterns.py' first, or using defaults")
        return patterns
    
    try:
        for line in PATTERNS_PATH.read_text(encoding='utf-8').split('\n'):
            line = line.strip()
            if not line:
                continue
            try:
                record = json.loads(line)
                rtype = record.get('type', '')
                
                if rtype == 'pattern.keywords':
                    patterns['keywords'] = record.get('keywords', [])
                elif rtype == 'pattern.topics':
                    patterns['topics'] = record.get('topics', {})
                elif rtype == 'pattern.tones':
                    patterns['tones'] = record.get('tones', {})
                elif rtype == 'pattern.entities':
                    patterns['entities'] = record.get('entities', [])
                elif rtype == 'pattern.phrases':
                    patterns['phrases'] = record.get('phrases', [])
                elif rtype == 'pattern.emojis':
                    patterns['emojis'] = record.get('emojis', [])
            except json.JSONDecodeError:
                continue
        
        print(f"Loaded patterns: {len(patterns['keywords'])} keywords, {len(patterns['topics'])} topics, {len(patterns['entities'])} entities")
    except Exception as e:
        print(f"Warning: Error loading patterns: {e}")
    
    return patterns


# Load patterns at module level
PATTERNS = load_patterns()

# Default patterns if none loaded
DEFAULT_TOPIC_KEYWORDS = {
    'technical': ['code', 'api', 'function', 'error', 'bug', 'server', 'database'],
    'emotional': ['feel', 'feeling', 'emotion', 'pain', 'fear', 'anxiety', 'happy', 'sad'],
    'creative': ['story', 'character', 'write', 'writing', 'creative', 'fiction'],
    'philosophical': ['consciousness', 'awareness', 'meaning', 'existence', 'reality', 'truth'],
    'problem_solving': ['problem', 'solve', 'solution', 'debug', 'fix', 'issue', 'challenge', 'approach']
}

DEFAULT_TONE_PATTERNS = {
    'personal': ['I feel', 'my experience', 'my life'],
    'distressed': ['overwhelmed', 'struggling', 'difficult', 'pain'],
    'playful': ['haha', 'lol', 'fun', 'joke'],
    'philosophical': ['meaning', 'existence', 'reality', 'truth'],
    'analytical': ['analyze', 'consider', 'think about', 'examine', 'evaluate']
}

# Use loaded patterns or defaults
TOPIC_KEYWORDS = PATTERNS['topics'] if PATTERNS['topics'] else DEFAULT_TOPIC_KEYWORDS
TONE_PATTERNS = PATTERNS['tones'] if PATTERNS['tones'] else DEFAULT_TONE_PATTERNS


def parse_timestamp(ts_str: str) -> Optional[float]:
    """Parse timestamp from various formats."""
    if not ts_str or ts_str == "Unknown Time":
        return None
    
    try:
        dt = datetime.fromisoformat(ts_str.replace('Z', '+00:00'))
        return dt.timestamp()
    except:
        try:
            return float(ts_str)
        except:
            return None


def extract_messages_from_jsonl(file_path: Path) -> List[Dict]:
    """Extract messages from JSONL file."""
    messages = []
    
    try:
        for line in file_path.read_text(encoding='utf-8').split('\n'):
            line = line.strip()
            if not line:
                continue
            
            try:
                msg = json.loads(line)
                timestamp = parse_timestamp(msg.get('timestamp', '')) if 'timestamp' in msg else None
                
                messages.append({
                    'role': msg.get('role', 'unknown'),
                    'timestamp': timestamp,
                    'timestamp_str': msg.get('timestamp', ''),
                    'content': msg.get('content', '')
                })
            except json.JSONDecodeError:
                continue
    except Exception as e:
        print(f"Error reading {file_path}: {e}")
    
    return messages


def detect_topic_tags(content: str) -> Set[str]:
    """Detect topic tags from content."""
    content_lower = content.lower()
    tags = set()
    
    for topic, keywords in TOPIC_KEYWORDS.items():
        if isinstance(keywords, list):
            for keyword in keywords:
                if keyword.lower() in content_lower:
                    tags.add(topic)
                    break
    
    return tags


def detect_tone_tags(content: str) -> Set[str]:
    """Detect tone tags from content."""
    content_lower = content.lower()
    tags = set()
    
    for tone, patterns in TONE_PATTERNS.items():
        if isinstance(patterns, list):
            for pattern in patterns:
                if pattern.lower() in content_lower:
                    tags.add(tone)
                    break
    
    if not tags:
        tags.add('neutral')
    
    return tags


def analyze_conversation(conversation_id: str, file_path: str, messages: List[Dict]) -> Dict:
    """
    Analyze a conversation for human communication patterns.
    
    Focus: Human identity fingerprinting - topics, tones, communication structure.
    """
    if not messages:
        return {
            'conversation_id': conversation_id,
            'file_path': file_path,
            'earliest_timestamp': None,
            'latest_timestamp': None,
            'message_count': 0,
            'user_message_count': 0,
            'topic_tags': [],
            'tone_tags': [],
            'truncated_or_corrupted': True,
            'messages_preview': []
        }
    
    # Sort by timestamp
    valid_messages = [m for m in messages if m.get('timestamp') is not None]
    if valid_messages:
        valid_messages.sort(key=lambda x: x['timestamp'] or 0)
    else:
        valid_messages = messages
    
    earliest_ts = valid_messages[0]['timestamp'] if valid_messages else None
    latest_ts = valid_messages[-1]['timestamp'] if valid_messages else None
    
    # Focus on human messages for pattern detection
    user_messages = [m for m in valid_messages if m.get('role') == 'user']
    
    # Detect patterns from human messages
    topic_tags = set()
    tone_tags = set()
    for msg in user_messages:
        topic_tags.update(detect_topic_tags(msg['content']))
        tone_tags.update(detect_tone_tags(msg['content']))
    
    # Check truncation
    truncated = False
    if messages:
        last_msg = messages[-1]
        content = last_msg.get('content', '')
        if content.endswith('...') or '[truncated]' in content.lower():
            truncated = True
        elif not content.strip():
            truncated = True
    
    # Preview (include both roles for context)
    messages_preview = []
    for msg in valid_messages[:15]:
        preview = msg['content'][:250]
        if len(msg['content']) > 250:
            preview += '...'
        messages_preview.append({
            'role': msg['role'],
            'timestamp': msg.get('timestamp_str', ''),
            'content_preview': preview
        })
    
    return {
        'conversation_id': conversation_id,
        'file_path': file_path,
        'earliest_timestamp': earliest_ts,
        'latest_timestamp': latest_ts,
        'message_count': len(valid_messages),
        'user_message_count': len(user_messages),
        'topic_tags': sorted(list(topic_tags)),
        'tone_tags': sorted(list(tone_tags)),
        'truncated_or_corrupted': truncated,
        'messages_preview': messages_preview
    }


def identify_key_events(conversation_id: str, file_path: str, 
                        messages: List[Dict], analysis: Dict) -> List[Dict]:
    """
    Identify key human communication events.
    
    Focus: Problem-solving moments, communication tempo changes, topic transitions, tone shifts.
    """
    events = []
    
    if not messages:
        return events
    
    valid_messages = [m for m in messages if m.get('timestamp') is not None]
    if valid_messages:
        valid_messages.sort(key=lambda x: x['timestamp'] or 0)
    else:
        valid_messages = messages
    
    # Focus on human messages
    user_messages = [m for m in valid_messages if m.get('role') == 'user']
    
    if len(user_messages) < 2:
        return events
    
    # 1. Problem-solving moments: Long, complex human messages with problem-solving keywords
    problem_keywords = ['problem', 'solve', 'solution', 'debug', 'fix', 'issue', 'challenge', 
                       'approach', 'strategy', 'how to', 'figure out', 'understand']
    for msg in user_messages:
        content_lower = msg['content'].lower()
        has_problem_keywords = any(kw in content_lower for kw in problem_keywords)
        is_complex = len(msg['content']) > 300  # Longer messages often indicate complex thinking
        
        if has_problem_keywords and is_complex:
            events.append({
                'event_type': 'PROBLEM_SOLVING',
                'timestamp': msg['timestamp'],
                'conversation_id': conversation_id,
                'file_path': file_path,
                'short_description': f"Complex problem-solving discussion. Length: {len(msg['content'])} chars. Preview: {msg['content'][:150]}..."
            })
            break  # One per conversation
    
    # 2. Communication tempo changes: Sudden shift in message length
    if len(user_messages) >= 3:
        lengths = [len(m['content']) for m in user_messages]
        avg_length = sum(lengths) / len(lengths)
        
        # Check for significant deviation (2x or 0.5x average)
        for i, msg in enumerate(user_messages[1:], 1):
            prev_length = len(user_messages[i-1]['content'])
            curr_length = len(msg['content'])
            
            if prev_length > 0:
                ratio = curr_length / prev_length
                # Significant change (2x longer or 0.5x shorter)
                if ratio >= 2.0 or ratio <= 0.5:
                    events.append({
                        'event_type': 'TEMPO_CHANGE',
                        'timestamp': msg['timestamp'],
                        'conversation_id': conversation_id,
                        'file_path': file_path,
                        'short_description': f"Communication tempo shift: {prev_length} → {curr_length} chars. Preview: {msg['content'][:150]}..."
                    })
                    break
    
    # 3. Topic transitions: Human shifts between different topic areas
    if len(user_messages) >= 2:
        prev_topics = detect_topic_tags(user_messages[0]['content'])
        for i, msg in enumerate(user_messages[1:], 1):
            curr_topics = detect_topic_tags(msg['content'])
            # Significant topic change (no overlap)
            if prev_topics and curr_topics and not (prev_topics & curr_topics):
                events.append({
                    'event_type': 'TOPIC_TRANSITION',
                    'timestamp': msg['timestamp'],
                    'conversation_id': conversation_id,
                    'file_path': file_path,
                    'short_description': f"Topic shift: {', '.join(prev_topics)} → {', '.join(curr_topics)}. Preview: {msg['content'][:150]}..."
                })
                break
            prev_topics = curr_topics
    
    # 4. Tone shifts: Emotional pattern changes
    if len(user_messages) >= 2:
        prev_tones = detect_tone_tags(user_messages[0]['content'])
        for i, msg in enumerate(user_messages[1:], 1):
            curr_tones = detect_tone_tags(msg['content'])
            # Significant tone change (no overlap, excluding neutral)
            prev_non_neutral = prev_tones - {'neutral'}
            curr_non_neutral = curr_tones - {'neutral'}
            if prev_non_neutral and curr_non_neutral and not (prev_non_neutral & curr_non_neutral):
                events.append({
                    'event_type': 'TONE_SHIFT',
                    'timestamp': msg['timestamp'],
                    'conversation_id': conversation_id,
                    'file_path': file_path,
                    'short_description': f"Tone shift: {', '.join(prev_non_neutral)} → {', '.join(curr_non_neutral)}. Preview: {msg['content'][:150]}..."
                })
                break
            prev_tones = curr_tones
    
    return events


def process_conversations(conversations_dir: Path) -> Tuple[List[Dict], List[Dict]]:
    """Process all conversations."""
    index_entries = []
    all_events = []
    
    jsonl_files = sorted(conversations_dir.glob('conversation_*.jsonl'))
    
    print(f"Processing {len(jsonl_files)} conversation files...")
    
    for jsonl_file in jsonl_files:
        conversation_id = jsonl_file.stem.replace('conversation_', '')
        messages = extract_messages_from_jsonl(jsonl_file)
        
        analysis = analyze_conversation(conversation_id, str(jsonl_file.relative_to(PROJECT_ROOT)), messages)
        index_entries.append(analysis)
        
        events = identify_key_events(conversation_id, str(jsonl_file.relative_to(PROJECT_ROOT)), messages, analysis)
        all_events.extend(events)
        
        if len(index_entries) % 100 == 0:
            print(f"  Processed {len(index_entries)} conversations...")
    
    return index_entries, all_events


def main():
    """Main function."""
    conversations_dir = get_user_dir(PROJECT_ROOT / 'conversations', USER_ID)
    memory_dir = MEMORY_DIR
    
    print("=" * 60)
    print("Building Interaction Map")
    print("=" * 60)
    print("Focus: Human communication patterns and identity fingerprinting")
    print()
    
    if not conversations_dir.exists():
        print(f"Error: Conversations directory not found: {conversations_dir}")
        return
    
    memory_dir.mkdir(exist_ok=True)
    
    index_entries, all_events = process_conversations(conversations_dir)
    
    # Sort by timestamp
    index_entries.sort(key=lambda x: x['earliest_timestamp'] or 0)
    all_events.sort(key=lambda x: x['timestamp'] or 0)
    
    # Write outputs to memory/
    index_path = memory_dir / 'interaction_map_index.json'
    with open(index_path, 'w', encoding='utf-8') as f:
        json.dump(index_entries, f, indent=2, ensure_ascii=False)
    print(f"\n✅ Index saved: {index_path} ({len(index_entries)} conversations)")
    
    events_path = memory_dir / 'interaction_key_events.json'
    with open(events_path, 'w', encoding='utf-8') as f:
        json.dump(all_events, f, indent=2, ensure_ascii=False)
    print(f"✅ Events saved: {events_path} ({len(all_events)} events)")
    
    # Summary
    problem_solving = sum(1 for e in all_events if e['event_type'] == 'PROBLEM_SOLVING')
    tempo_changes = sum(1 for e in all_events if e['event_type'] == 'TEMPO_CHANGE')
    topic_transitions = sum(1 for e in all_events if e['event_type'] == 'TOPIC_TRANSITION')
    tone_shifts = sum(1 for e in all_events if e['event_type'] == 'TONE_SHIFT')
    
    print()
    print("=" * 60)
    print("SUMMARY")
    print("=" * 60)
    print(f"Total conversations: {len(index_entries)}")
    print(f"Total human messages: {sum(e['user_message_count'] for e in index_entries)}")
    print(f"Key events detected: {len(all_events)}")
    print(f"  - Problem-solving moments: {problem_solving}")
    print(f"  - Communication tempo changes: {tempo_changes}")
    print(f"  - Topic transitions: {topic_transitions}")
    print(f"  - Tone shifts: {tone_shifts}")


if __name__ == '__main__':
    main()

