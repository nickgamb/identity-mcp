# docker exec ollama-server ollama list
# docker exec ollama-server ollama pull gpt-oss:20b
# docker cp ollama-server:/root/.ollama/models/blobs ollama_models/
services:
  mcp-server:
    build: .
    container_name: mcp-server
    ports:
      - "4000:4000"
    environment:
      - PORT=4000
      - PROJECT_ROOT=/app
      - MEMORY_DIR=/app/memory
      - FILES_DIR=/app/files
      - IDENTITY_SERVICE_URL=http://identity-service:4001
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./memory:/app/memory
      - ./conversations:/app/conversations
      - ./files:/app/files
      - ./scripts:/app/scripts
      - ./models:/app/models
      - ./training_data:/app/training_data
      - ./adapters:/app/adapters
    networks:
      - mcp-network
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:4000/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    depends_on:
      - ollama

  ollama:
    image: ollama/ollama:latest
    container_name: ollama-server
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
      # Mount local ollama_models to persist downloaded models (read-write so new models can be saved)
      # Models are stored in /home/<USERNAME>/models/ollama_models on the host
      # CHANGE <USERNAME> TO YOUR OWN USERNAME
      - /home/<USERNAME>/models/ollama_models/blobs:/root/.ollama/models/blobs
      - /home/<USERNAME>/models/ollama_models/manifests:/root/.ollama/models/manifests
      # Mount adapters directory so LoRA adapters are accessible from inside the container
      # CHANGE <PROJECT_PATH> to your project location (e.g., /home/<USERNAME>/ai)
      - /home/<USERNAME>/<PROJECT_PATH>/adapters:/app/adapters:ro
    networks:
      - mcp-network
    # Enable GPU support for NVIDIA - use both GPUs explicitly
    # NOTE: Ollama doesn't split a single model across multiple GPUs.
    # Each model runs on one GPU. For multi-GPU inference, use the HF service instead.
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_NUM_PARALLEL=1  # Number of parallel requests (can increase for multiple models)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD-SHELL", "timeout 1 bash -c '</dev/tcp/localhost/11434' || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 120s

  # Identity Verification Service with Semantic Embeddings
  # Uses sentence-transformers for deep semantic identity verification
  # 
  # NOTE: This service uses a Docker Compose profile so it doesn't start by default.
  # To start it: docker-compose --profile identity up -d identity-service
  # Or start with all optional services: docker-compose --profile identity --profile hf up -d
  #
  # When running, the MCP will automatically use it for enhanced verification.
  # If not running, MCP falls back to stylistic-only verification.
  identity-service:
    profiles: ["identity"]  # Only start when --profile identity is used
    build:
      context: ./identity_service
      dockerfile: Dockerfile
    container_name: identity-service
    ports:
      - "4001:4001"
    environment:
      - PORT=4001
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./models:/app/models
    networks:
      - mcp-network
    runtime: nvidia  # Enable GPU support for faster embeddings
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:4001/health')"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # HuggingFace Inference Service with Dynamic Model Hot-Swapping
  # Supports multiple models and adapters with automatic hot-swapping
  # 
  # To start: docker-compose --profile hf up -d hf-service
  #
  # Directory structure (CHANGE PATHS TO MATCH YOUR SETUP):
  #   /home/<USERNAME>/models/hf_models/  → /app/models/   (all HF models)
  #   /home/<USERNAME>/<PROJECT_PATH>/adapters/ → /app/adapters/ (all LoRA adapters)
  #
  # To add new models/adapters:
  #   1. Place model in /home/<USERNAME>/models/hf_models/<model-name>/
  #   2. Place adapter in /home/<USERNAME>/<PROJECT_PATH>/adapters/<adapter-name>/
  #   3. Add entry to MODEL_REGISTRY in hf_api.py
  #   4. Add to librechat.yaml modelSpecs and HFService models list
  hf-service:
    profiles: ["hf"]  # Only start when --profile hf is used
    build:
      context: ./hf-service
      dockerfile: Dockerfile
    container_name: hf-service
    ports:
      - "8000:8000"
    environment:
      - HF_KEEP_ALIVE=300  # Unload after 5 minutes of inactivity (0=immediate, -1=never)
      - NVIDIA_VISIBLE_DEVICES=all
      # Base paths for models and adapters (parent directories)
      - HF_MODELS_PATH=/app/models
      - HF_ADAPTERS_PATH=/app/adapters
    volumes:
      # Mount all HF models - accessible at /app/models/<model-name>
      # CHANGE <USERNAME> TO YOUR OWN USERNAME
      - /home/<USERNAME>/models/hf_models:/app/models:ro
      # Mount all adapters - accessible at /app/adapters/<adapter-name>
      # CHANGE <PROJECT_PATH> to your project location
      - /home/<USERNAME>/<PROJECT_PATH>/adapters:/app/adapters:ro
    networks:
      - mcp-network
    # Enable GPU support - use both GPUs explicitly
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 10s  # Reduced since model loads lazily on first request

  # LibreChat Dependencies
  mongodb:
    image: mongo:7.0
    container_name: librechat-mongodb
    restart: unless-stopped
    volumes:
      - mongodb-data:/data/db
    networks:
      - mcp-network
    environment:
      - MONGO_INITDB_ROOT_USERNAME=librechat
      - MONGO_INITDB_ROOT_PASSWORD=librechat
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh localhost:27017/test --quiet
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  redis:
    image: redis:7-alpine
    container_name: librechat-redis
    restart: unless-stopped
    volumes:
      - redis-data:/data
    networks:
      - mcp-network
    command: redis-server --appendonly yes

  # LibreChat API
  librechat-api:
    image: ghcr.io/danny-avila/librechat:latest
    container_name: librechat-api
    restart: unless-stopped
    ports:
      - "3080:3080"
    environment:
      - NODE_ENV=production
      - MONGO_URI=mongodb://librechat:librechat@mongodb:27017/LibreChat?authSource=admin
      - REDIS_URI=redis://redis:6379
      - MEILI_HOST=http://meilisearch:7700
      - MEILI_MASTER_KEY=masterKey
      - JWT_SECRET=mcp-demo-secret-key-change-in-production
      - JWT_REFRESH_SECRET=mcp-demo-refresh-secret-change-in-production
      - JWT_EXPIRES_IN=1d
      - JWT_REFRESH_EXPIRES_IN=7d
      - DOMAIN_CLIENT=${LIBRECHAT_DOMAIN:-http://localhost:3080}
      - DOMAIN_SERVER=${LIBRECHAT_DOMAIN:-http://localhost:3080}
      - ALLOW_REGISTRATION=true
      - ALLOW_SOCIAL_LOGIN=false
      - APP_TITLE=Chat
      - ENABLE_AGENTS=false
      - LANGGRAPH_RECURSION_LIMIT=50000
    volumes:
      - ./librechat-config/librechat.yaml:/app/librechat.yaml:ro
      - librechat-uploads:/app/client/public/uploads
    networks:
      - mcp-network
    depends_on:
      mongodb:
        condition: service_healthy
      redis:
        condition: service_started
      meilisearch:
        condition: service_started
      ollama:
        condition: service_started
      # hf-service is optional (uses profile) - removed from depends_on
      # If you need it, start with: docker-compose --profile hf up -d hf-service
      mcp-server:
        condition: service_healthy

  # Meilisearch for LibreChat search
  meilisearch:
    image: getmeili/meilisearch:v1.5
    container_name: librechat-meilisearch
    restart: unless-stopped
    volumes:
      - meilisearch-data:/meili_data
    networks:
      - mcp-network
    environment:
      - MEILI_MASTER_KEY=masterKey
      - MEILI_ENV=development

  # Identity MCP Dashboard
  dashboard:
    build:
      context: ./dashboard
      dockerfile: Dockerfile
    container_name: identity-dashboard
    ports:
      - "3001:3001"
    environment:
      - VITE_OIDC_ENABLED=false
    volumes:
      - ./dashboard:/app
      - /app/node_modules
    networks:
      - mcp-network
    depends_on:
      mcp-server:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3001"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s

networks:
  mcp-network:
    driver: bridge

volumes:
  ollama-data:
    driver: local
  hf-cache:
    driver: local
  mongodb-data:
    driver: local
  redis-data:
    driver: local
  meilisearch-data:
    driver: local
  librechat-uploads:
    driver: local
