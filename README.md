# Identity MCP

A behavioral identity verification system that creates an "identity fingerprint" from your conversation history. Train a model on how you communicate, then verify if new messages match your identity.

## What It Does

- **Parses** your ChatGPT formatted conversation export (conversations.json, memories.json)
- **Discovers** patterns unique to you (topics, vocabulary, style)
- **Trains** an embedding model on your communication patterns
- **Verifies** new messages against your identity fingerprint
- **Serves** identity data via MCP protocol (for LibreChat, etc.)
- **Multi-user and OIDC**: Supports OIDC authentication with complete data isolation per user.

## Screenshots

### Identity Model Visualizations
![Identity Model Visualizations](screenshots/vibes_compressed.gif)
*Interactive charts showing identity evolution, stylistic profiles, vocabulary analysis, and temporal patterns*

### Pipeline Dashboard
![Pipeline Dashboard](screenshots/dashboard1.png)
*Run processing scripts and monitor real-time output*

### Data Explorer - Status & Upload
![Data Explorer](screenshots/data_explorer.png)
*Upload conversations and memories, see completion status*

### Data Explorer - Conversations
![Conversations Browser](screenshots/data_explorer_conversations.png)
*Browse, search, and edit all parsed conversations*

### Data Explorer - Memories
![Memories Browser](screenshots/data_explorer_memories.png)
*View and manage all memory files with Monaco editor*

### Data Explorer - Identity Analysis
![Identity Analysis](screenshots/data_explorer_memories_analysis.png)
*Visualize your identity model with stylistic profiles and vocabulary*

### Data Explorer - Files
![Files Manager](screenshots/data_explorer_files.png)
*Upload, view, edit, and delete files for RAG access*

### Chat Interface (LibreChat)
![Chat](screenshots/chat.png)
*Use your identity data in conversations via MCP*

### Docker Compose
![Docker Compose](screenshots/compose.png)
*Full stack deployment with GPU support*

## Quick Start

### Option 1: Dashboard (Recommended for First-Time Setup)

```bash
# 1. Start services
npm install
npm start                    # MCP server on :4000

# In another terminal
cd dashboard
npm install
npm run dev                  # Dashboard on :3001

# 2. Open http://localhost:3001
# 3. Upload your conversations.json and memories.json via UI
# 4. Click "Run" on each pipeline script in order
# 5. Monitor progress in real-time
```

### Option 2: Command Line

```bash
# 1. Export your data from ChatGPT and place in project
cp ~/Downloads/conversations.json conversations/
cp ~/Downloads/memories.json memory/

# 2. Process your data
cd scripts/conversation_processing
python parse_conversations.py
python analyze_patterns.py
python parse_memories.py
python analyze_identity.py
python build_interaction_map.py

# 3. Train identity model
cd ../identity_model
python train_identity_model.py

# 4. Start services
cd ../..
docker-compose up -d                              # MCP only
docker-compose --profile identity up -d           # MCP + Identity Service
```

## Documentation

| Doc | Description |
|-----|-------------|
| [Getting Started](docs/GETTING_STARTED.md) | Full setup guide with all options |
| [MCP Protocol Reference](docs/MCP_README.md) | Complete API reference for all 50 MCP tools |
| [Identity Verification](docs/IDENTITY_VERIFICATION.md) | How the verification system works |
| [Multi-User & OIDC Support](docs/MULTI_USER_OIDC.md) | Multi-user data isolation and OIDC authentication |
| [Docker Setup](docs/DOCKER_SETUP.md) | Container deployment guide |
| [Environment Variables](docs/ENVIRONMENT_VARIABLES.md) | Complete reference for all configuration options |

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         END-TO-END FLOW                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  RAW DATA (from ChatGPT export)                                         â”‚
â”‚  â”œâ”€â”€ conversations.json                                                  â”‚
â”‚  â””â”€â”€ memories.json                                                       â”‚
â”‚           â”‚                                                              â”‚
â”‚           â–¼                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  PROCESSING SCRIPTS (scripts/conversation_processing/)          â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  1. parse_conversations.py                                       â”‚    â”‚
â”‚  â”‚     â†’ conversations/*.jsonl (parsed messages)                    â”‚    â”‚
â”‚  â”‚     â†’ conversations/*.md (human-readable)                        â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  2. analyze_patterns.py                                          â”‚    â”‚
â”‚  â”‚     â†’ memory/identity.jsonl (core identity patterns)             â”‚    â”‚
â”‚  â”‚     â†’ memory/patterns.jsonl (keywords, topics, entities)         â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  3. parse_memories.py                                            â”‚    â”‚
â”‚  â”‚     â†’ memory/user.context.jsonl (ChatGPT memories as context)    â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  4. analyze_identity.py                                          â”‚    â”‚
â”‚  â”‚     â†’ memory/identity_analysis.jsonl (relational/stylistic)      â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  5. build_interaction_map.py                                     â”‚    â”‚
â”‚  â”‚     â†’ memory/interaction_map_index.json (searchable index)       â”‚    â”‚
â”‚  â”‚     â†’ memory/interaction_key_events.json (human communication events) â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚           â”‚                                                              â”‚
â”‚           â–¼                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  IDENTITY MODEL TRAINING (scripts/identity_model/)              â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  train_identity_model.py                                         â”‚    â”‚
â”‚  â”‚     READS:                                                       â”‚    â”‚
â”‚  â”‚       â€¢ conversations/*.jsonl (user messages)                    â”‚    â”‚
â”‚  â”‚       â€¢ memory/patterns.jsonl (boosts distinctive terms)         â”‚    â”‚
â”‚  â”‚       â€¢ memory/identity.jsonl (identity phrases)                 â”‚    â”‚
â”‚  â”‚       â€¢ memory/identity_analysis.jsonl (relational markers)      â”‚    â”‚
â”‚  â”‚       â€¢ memory/user.context.jsonl (ChatGPT memories)             â”‚    â”‚
â”‚  â”‚     OUTPUTS:                                                     â”‚    â”‚
â”‚  â”‚       models/identity/                                           â”‚    â”‚
â”‚  â”‚       â”œâ”€â”€ config.json (model info, thresholds, signals)          â”‚    â”‚
â”‚  â”‚       â”œâ”€â”€ identity_centroid.npy (semantic "fingerprint")         â”‚    â”‚
â”‚  â”‚       â”œâ”€â”€ stylistic_profile.json (how you write)                 â”‚    â”‚
â”‚  â”‚       â””â”€â”€ vocabulary_profile.json (words + identity-boosted)     â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚           â”‚                                                              â”‚
â”‚           â–¼                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  RUNTIME SERVICES                                                â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚
â”‚  â”‚  â”‚  MCP Server (:4000)  â”‚â—„â”€â”€â–¶â”‚ Identity Service (:4001)       â”‚ â”‚    â”‚
â”‚  â”‚  â”‚                      â”‚    â”‚                                â”‚ â”‚    â”‚
â”‚  â”‚  â”‚ â€¢ memory/*.jsonl     â”‚    â”‚ â€¢ Loads trained model          â”‚ â”‚    â”‚
â”‚  â”‚  â”‚ â€¢ files/* (RAG)      â”‚    â”‚ â€¢ Sentence transformer         â”‚ â”‚    â”‚
â”‚  â”‚  â”‚ â€¢ conversations/*    â”‚    â”‚ â€¢ Computes similarity to       â”‚ â”‚    â”‚
â”‚  â”‚  â”‚ â€¢ Stylistic check    â”‚    â”‚   identity centroid            â”‚ â”‚    â”‚
â”‚  â”‚  â”‚   (fallback)         â”‚    â”‚                                â”‚ â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚           â”‚                                                              â”‚
â”‚           â–¼                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  VERIFICATION (during live conversation)                         â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  User sends message â†’ MCP calls identity_verify â†’                â”‚    â”‚
â”‚  â”‚    IF identity-service running:                                  â”‚    â”‚
â”‚  â”‚      60% semantic (distance from centroid)                       â”‚    â”‚
â”‚  â”‚      25% stylistic (punctuation, sentence length, etc)           â”‚    â”‚
â”‚  â”‚      15% vocabulary (distinctive words)                          â”‚    â”‚
â”‚  â”‚    ELSE fallback:                                                â”‚    â”‚
â”‚  â”‚      60% stylistic + 40% vocabulary                              â”‚    â”‚
â”‚  â”‚                                                                  â”‚    â”‚
â”‚  â”‚  Returns: { verified: true/false, confidence: high/medium/low }  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## MCP Tools (50 total)

The MCP server exposes tools for:
- **Memory** (7 tools) - Read/write/search identity memories
- **Conversations** (4 tools) - Query parsed conversation history  
- **Identity Analysis** (5 tools) - Relational patterns, naming events, momentum
- **Interaction Map** (5 tools) - Human communication patterns, topic/tone analysis, event timeline
- **Identity Verification** (4 tools) - Verify messages against your fingerprint
- **Files** (4 tools) - RAG over your documents
- **Fine-tuning** (5 tools) - LoRA training and dataset export
- **Pipeline** (5 tools) - Run processing scripts, check status
- **Data Management** (9 tools) - Upload, browse, edit, and clean data
- **Statistics** (2 tools) - Memory and conversation analytics

See [docs/GETTING_STARTED.md](docs/GETTING_STARTED.md) for full tool reference.

## Dashboard Features

The web dashboard provides:
- **Upload** - Drag & drop conversations.json and memories.json
- **Status** - Visual indicators for source files and generated data
- **Pipeline** - Run all processing scripts with real-time output
- **Data Explorer** - Browse, search, and edit:
  - All conversations (with VS Code-like Monaco editor)
  - All memory files (with full CRUD operations)
  - Files directory (view and manage)
- **Clean** - Safely remove generated data (keeps source files)

Access at: http://localhost:3001 (after starting dashboard)

## Requirements

- **Node.js 18+** (MCP server)
- **Python 3.9+** (processing scripts, identity service)
- **GPU recommended** for training (works on CPU, just slower)

## Roadmap

| Status | Feature |
|--------|---------|
| âœ… | Parse ChatGPT conversation exports |
| âœ… | Discover identity patterns (vocabulary, style, topics) |
| âœ… | Train semantic embedding model |
| âœ… | Identity verification via MCP tools |
| âœ… | Memory files enhance training (boosts distinctive terms) |
| âœ… | Multi-user support with OIDC authentication and data isolation |
| âš ï¸ | Enroll and Train a model on EEG signal data from EMOTIV and PiEEG |
| âš ï¸ | Live EEG verification assurance signals based on EEG model |
| ğŸ”² | **Non-conversational data support** - Train on essays, journals, emails, blog posts, social media |
| ğŸ”² | Multiple identity profiles (compare/switch between identities) |
| ğŸ”² | Identity drift detection (alert when patterns change over time) |
| ğŸ”² | Export identity model for use in other systems |

#License

Apache License 2.0
See the [LICENSE.md](LICENSE.md) file for full details.